# Dynamic ETL Backend (Node.js + Express + MongoDB)
---

## ğŸ“Œ Overview

The backend is designed to handle messy, unpredictable, and evolving data by:

* Accepting **any file format** (PDF, CSV, DOCX, TXT, HTML, JSON, images, mixed-format files)
* Extracting meaningful data intelligently
* Generating schemas automatically
* Creating **new schema versions** on structural changes
* Storing both **raw and normalized data**
* Tracking schema evolution internally

It forms the **core ETL engine**, handling all extraction, transformation, and storage operations.

---

## ğŸ—ï¸ Backend Architecture

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  File Ingestion  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Content Classifier    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Multi-Format Extraction Layer â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“               â†“
   PDF Extractor      CSV Extractor    DOCX Extractor   Image OCR   HTML/Text Parser
          â†“               â†“                    â†“             â†“            â†“
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Extracted Unified Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Dynamic Schema Gen â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Schema Drift Check â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Schema Versioning  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Raw + Normalized Storage   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Key Backend Features

### 1ï¸âƒ£ Accepts Any File Format

* Uses **content-based detection**, NOT extension-based
* Supported formats:

  * PDFs, CSVs, DOCX, TXT, HTML, JSON, Images
  * Mixed-format files (e.g., HTML + images + JSON)
* Detects format via MIME type, magic bytes, and content patterns

---

### 2ï¸âƒ£ Multi-Layer Extraction

* Specialized extractors per format:

  * **PDF:** text + metadata
  * **CSV:** rows + headers
  * **DOCX:** paragraphs + tables
  * **HTML:** cleaned text + tags
  * **Images:** OCR text
* Mixed files are processed segment-by-segment
* Extracted data **merged into a unified JSON object**

---

### 3ï¸âƒ£ Dynamic Schema Inference

* Scans extracted JSON-like data
* Detects fields, data types, optional vs required, nested structures
* Handles inconsistent fields and type variations
* Example inferred schema:

```json
{
  "title": "string",
  "amount": "float",
  "timestamp": "datetime",
  "images_text": "array"
}
```

---

### 4ï¸âƒ£ Schema Drift Detection

* Compares new schema to the latest stored version
* Detects:

  * Added or removed fields
  * Data type changes
  * Nested structure changes
* Creates **new schema version** automatically when changes occur

---

### 5ï¸âƒ£ Schema Version Control

* Stores every schema version with:

  * Version number
  * Schema structure
  * Timestamp
  * Diff from previous version
* Ensures **safe storage**, but **does not provide full backward query compatibility** yet
* Example version history:

```
v1 â†’ name, email  
v2 â†’ + html_text  
v3 â†’ + ocr_results  
v4 â†’ data type change in "amount"
```

---

### 6ï¸âƒ£ Raw + Normalized Storage

* **Raw Storage:** exact uploaded content + extraction outputs
* **Normalized Storage:** data cleaned and transformed according to inferred schema
* **Schema Metadata:** current version details
* **Schema History:** tracks schema versions internally

---

### 7ï¸âƒ£ Error Handling & Fault Tolerance

* Failed files logged for retries
* Raw content stored for debugging
* Extraction errors do **not block ingestion**

---

## ğŸš€ Installation

```bash
git clone https://github.com/yourrepo/dynamic-etl.git
cd backend
npm install
node app.js
```

---

## ğŸ“Œ API Endpoints

* **POST /upload** â†’ Upload a file
* **GET /schema/latest** â†’ Get latest schema version
* **GET /schema/versions** â†’ Get internal schema history
* **GET /records** â†’ Fetch normalized stored records
* **GET /stats** â†’ Pipeline statistics

---

## ğŸ§© Tech Stack (Backend Only)

* **Node.js + Express** â†’ backend server
* **Multer** â†’ file uploads
* **pdf-parse** â†’ PDF extraction
* **PapaParse** â†’ CSV parsing
* **Tesseract.js** â†’ OCR for images
* **Mammoth** â†’ DOCX extraction
* **Cheerio** â†’ HTML parsing
* **MongoDB** â†’ dynamic storage + versioning

---

## âš¡ Backend Limitations (Current)

* âŒ No support for `.md` markdown
* âŒ No fragment-level counts, offsets, or key-value metadata
* âŒ Normalization is basic; mixed-type handling is limited
* âŒ No DB compatibility metadata or suggested indexes
* âŒ Schema migration / backward query support is partial
* âŒ No LLM / natural language query interface
* âŒ Minimal logging & security
* âŒ No stress/performance tests for large/concurrent uploads

---

## ğŸ† Why the Backend Stands Out

* Handles any data format, including mixed files
* Fully automated **dynamic schema generation**
* Maintains **internal schema version history**
* Stores **raw + normalized data** safely
* Acts as the **core ETL engine** for evolving, unstructured datasets

---


