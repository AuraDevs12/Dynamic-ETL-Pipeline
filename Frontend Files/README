---

# Dynamic-ETL-Pipeline

**AuraVerse Hackathon Project**

ğŸ§  **Dynamic ETL Pipeline for Unstructured & Multi-Format Data**
ğŸš€ *Auto-Schema Generation â€¢ Schema Versioning â€¢ Multi-Format Extraction â€¢ Dynamic Storage*

---

## ğŸ“Œ Overview

Modern data sourcesâ€”scrapers, logs, emails, PDFs, documents, user uploadsâ€”produce **messy, unpredictable data**.

Traditional ETL systems fail because they rely on **fixed schemas**, while real-world data **changes all the time**.

This project solves that by building a **Dynamic ETL Pipeline** capable of:

```
âœ” Accepting any file format (PDF, CSV, DOCX, HTML, JSON, TXT, images)
âœ” Extracting content intelligently based on actual file content (not extensions)
âœ” Dynamically inferring schema from data
âœ” Detecting schema drift automatically
âœ” Creating new schema versions when structure changes
âœ” Storing raw + normalized data with full version traceability
```

Perfect for real-world **unstructured**, **semi-structured**, and **evolving** datasets.

---

# ğŸ—ï¸ Architecture Diagram

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  File Ingestion  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Content Classifier    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Multi-Format Extraction Layer â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“               â†“
   PDF Extractor      CSV Extractor    DOCX Extractor   Image OCR   HTML/Text Parser
          â†“               â†“                    â†“             â†“            â†“
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Extracted Unified Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Dynamic Schema Gen â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Schema Drift Check â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Schema Versioning  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Raw + Normalized Storage   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ¯ Key Features

## 1ï¸âƒ£ Accepts Any File Format â€” Truly ANY

The backend uses **MIME detection + magic bytes**, not file extensions.

Supported formats:

* PDF
* CSV
* DOCX
* TXT
* HTML
* JSON
* JPG/PNG (OCR extraction using Tesseract.js)
* Multi-format embedded files (like HTML with JSON or images)

Even if a file contains multiple formats â†’ the system **segments and extracts all of them**.

---

## 2ï¸âƒ£ Multi-Format Extraction Engine

Each extractor is specialized:

| Format     | Tool           | Output             |
| ---------- | -------------- | ------------------ |
| PDF        | `pdf-parse`    | text, metadata     |
| CSV        | `PapaParse`    | rows, headers      |
| Images     | `Tesseract.js` | OCR text           |
| DOCX       | `Mammoth`      | paragraphs, tables |
| HTML       | `Cheerio`      | structured text    |
| Plain text | Native         | text               |
| JSON       | Native         | objects            |

All normalized into a **Unified Extracted Payload**.

---

## 3ï¸âƒ£ Dynamic Schema Inference

After extraction, the system:

* Scans all fields
* Detects nested structures
* Determines data types
* Marks optional fields
* Tracks frequency of appearance
* Handles arrays and objects

Example inferred schema:

```json
{
  "title": { "types": ["string"], "optional": false },
  "amount": { "types": ["number"], "optional": true },
  "ocrText": { "types": ["string"], "optional": true },
  "metadata.pageCount": { "types": ["integer"], "optional": false }
}
```

---

## 4ï¸âƒ£ Schema Drift Detection

A new schema version is created if:

* A field is added
* A field is removed
* A data type changes
* Nested object structure changes
* Arrays appear or disappear

This enables **full traceability** over time.

---

## 5ï¸âƒ£ Schema Version Control (Schema Registry)

Each version contains:

* `version`
* `fields`
* `optional fields`
* `totalSamples`
* `notes`
* `createdAt`
* `diff from previous version`

Example evolution:

```
v1 â†’ name, email  
v2 â†’ + extracted_html  
v3 â†’ + ocr_result  
v4 â†’ type change in "price"
```

---

## 6ï¸âƒ£ Dual Storage Layer

### âœ” **Raw Storage**

Stores exactly what was extracted.

### âœ” **Normalized Storage**

Mapped strictly to the schema versionâ€”clean, structured, standardized.

---

## 7ï¸âƒ£ Dashboard Features

Frontend shows:

* Total files ingested
* Schema versions
* File type distribution
* Extraction success/failure stats
* Raw + normalized data browser
* Schema comparison view
* Recent uploads

Built with:

* HTML
* CSS
* JavaScript (Vanilla)

---

# ğŸ§ª How the Pipeline Works (Step-by-Step)

```
1. User uploads one or multiple files
2. Multer stores them temporarily
3. System identifies file type using MIME + magic bytes
4. Correct extractor is activated (PDF, OCR, CSV, DOCX, etc.)
5. Extracted content is converted into a uniform JSON-like payload
6. Schema inference scans samples and infers structure
7. Schema drift checker compares with previous version
8. If changed â†’ new version stored
9. Raw record stored (payload + metadata)
10. Normalized record stored (schema-mapped)
11. Dashboard updates statistics and versions
```

---

# ğŸš€ Installation

Clone the project:

```bash
git clone https://github.com/yourrepo/dynamic-etl.git
cd backend
npm install
node app.js
```

Configure MongoDB in `.env`:

```
MONGO_URI=mongodb://localhost:27017/etl
PORT=3000
```

Run:

```bash
node app.js
```

---

# ğŸ“Œ API Endpoints

### POST `/upload`

Upload file(s) for ingestion.

### GET `/schema/latest`

Returns the latest active schema version.

### GET `/schema/versions`

Returns all schema versions.

### GET `/raw`

List raw stored records.

### GET `/normalized`

List normalized records.

### POST `/admin/run`

Force run schema inference + normalization.

### GET `/stats`

Pipeline statistics.

---

# ğŸ§© Tech Stack

### **Backend**

* Node.js
* Express
* Multer
* pdf-parse
* PapaParse
* Tesseract.js
* Mammoth
* Cheerio

### **Database**

* MongoDB
* BSON for flexible dynamic documents

### **Frontend**

* HTML5
* CSS
* Vanilla JavaScript

---

# ğŸ† Why This Project Stands Out

```
âœ” Works with ANY file format
âœ” Hardware-agnostic OCR pipeline
âœ” Fully dynamic schema inference
âœ” Automatic schema versioning
âœ” Handles schema drift like enterprise systems
âœ” Real-world ETL pipeline simulation
âœ” Mixed-format file support (rare feature)
```

---

# ğŸ¤ Contributors

```
Team AuraDevs â€¢ AuraVerse Hackathon 2025
- Amartya Majumder
- Bhumi N Deshpande
- Akash Patel
```

---


