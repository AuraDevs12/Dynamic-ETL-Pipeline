# Dynamic-ETL-Pipeline
AuraVerse Hackathon Project

ğŸ§  **Dynamic ETL Pipeline for Unstructured & Multi-Format Data**

### ğŸš€ Auto-Schema Generation â€¢ Schema Versioning â€¢ Multi-Format Extraction â€¢ Dynamic Storage and parsing Multi Format Data

---

## ğŸ“Œ **Overview**

Modern data sources (scrapers, logs, documents, user uploads) produce *messy and unpredictable data*.
Traditional ETL pipelines break because they require **fixed schemas**, but real-world data **changes frequently**.

This project solves that problem by building a **dynamic ETL pipeline** capable of:
```
âœ” Accepting *any file format* (PDF, CSV, DOCX, TXT, HTML, JSON, images, mixed-format files)
âœ” Extracting data intelligently
âœ” Generating schemas automatically
âœ” Creating new schema versions when structure changes
âœ” Storing both raw and normalized data
âœ” Tracking schema evolution over time

Perfect for real-world messy data environments.
```
---

# ğŸ—ï¸ **Architecture**

```
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  File Ingestion  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Content Classifier    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Multi-Format Extraction Layer â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“               â†“
   PDF Extractor      CSV Extractor    DOCX Extractor   Image OCR   HTML/Text Parser
          â†“               â†“                    â†“             â†“            â†“
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Extracted Unified Data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Dynamic Schema Gen â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Schema Drift Check â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Schema Versioning  â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â†“
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Raw + Normalized Storage   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ¯ **Key Features**

## **1ï¸âƒ£ Accepts Any File Format**

The backend handles *content-based detection*, NOT extension-based:

* PDFs
* CSVs
* Images (OCR)
* HTML
* Plain text
* DOCX
* JSON
* Even files that contain **multiple formats inside** (e.g., HTML + images + JSON)

* The backend inspects the fileâ€™s actual content (MIME type, magic bytes, and patterns) to determine format, rather than relying on the filename or extension.

---

## **2ï¸âƒ£ Multi-Layer Extraction Engine**

Each format uses a specialized extractor:

* PDF â†’ text + metadata
* CSV â†’ rows + headers
* HTML â†’ cleaned text + tags
* Images â†’ OCR text
* DOCX â†’ paragraphs + tables
* Mixed files â†’ processed segment-by-segment

All extracted into a **unified structured object**.
---

## **3ï¸âƒ£ Dynamic Schema Inference**

The pipeline:

* Scans extracted data
* Detects fields
* Infers data types
* Handles nested structures
* Normalizes inconsistent fields

Example inferred schema:

```json
{
  "title": "string",
  "amount": "float",
  "timestamp": "datetime",
  "images_text": "array"
}
```
---

## **4ï¸âƒ£ Schema Drift Detection**

Your backend compares the **newly inferred schema** with the **latest stored version**.

If ANY change is found:

* New field added
* Field removed
* Data type changed
* New nested structure appears

Then â†’ a new schema version is created.

---

## **5ï¸âƒ£ Schema Version Control (Registry)**

Every version includes:

* version number
* schema structure
* timestamp
* diff from previous version

Example:

```
v1 â†’ name, email  
v2 â†’ + html_text  
v3 â†’ + ocr_results  
v4 â†’ data type change in "amount"
```

This enables full trackability and partial backward compatibility.

---

## **6ï¸âƒ£ Raw + Normalized Storage Layers**

The backend stores:

### âœ” **Raw Storage**

Exact file content + extraction outputs.

### âœ” **Normalized Storage**

Cleanly transformed data mapped to the inferred schema.

### âœ” **Schema Metadata**

Current schema version and details.

### âœ” **Schema History**

Every version ever created.

---

## **7ï¸âƒ£ Frontend Dashboard**

Shows:

* Total files processed
* Schema evolution timeline
* File type distribution
* Number of records
* Extraction success/failure stats
* Number of versions
* Details

Built using:
**HTML5 + CSS + JavaScript**

---


# ğŸ§ª **How the Pipeline Works**
```
1ï¸âƒ£ User uploads any file
2ï¸âƒ£ System identifies the content inside it
3ï¸âƒ£ Extractors pull meaningful data
4ï¸âƒ£ Schema is generated from extracted fields
5ï¸âƒ£ Schema compared with previous version
6ï¸âƒ£ If changed â†’ new version created
7ï¸âƒ£ Data stored in raw + normalized format
8ï¸âƒ£ Dashboard updates automatically
```
---

# ğŸš€ **Installation**

```
git clone https://github.com/yourrepo/dynamic-etl.git
cd backend
npm install
node app.js
```

---

# ğŸ“Œ **API Endpoints**

### `POST /upload`

Upload any file.

### `GET /schema/latest`

Fetch latest schema version.

### `GET /schema/versions`

Get schema history.

### `GET /records`

Fetch normalized stored records.

### `GET /stats`

Pipeline statistics.

---

# ğŸ§© **Tech Stack**

* **HTML5**
* **CSS**
* **JavaScript** 
* **Node.js + Express** (backend server)
* **Multer** (file uploads)
* **pdf-parse** (PDF extraction)
* **PapaParse** (CSV parsing)
* **Tesseract.js** (OCR for images)
* **Mammoth** (DOCX extraction)
* **Cheerio** (HTML parsing)
* **MongoDB** (dynamic storage + versioning)

---

# ğŸ† **Why This Project Stands Out**
```
âœ” Can handle any data thrown at it
âœ” Fully automated schema evolution
âœ” Complete version history
âœ” Supports mixed-format files
âœ” Pipeline mimics real-world enterprise ETL systems
```
---

# ğŸ¤ **Contributors**
```
Team AuraDevs â€¢ AuraVerse â€¢ 2025
Amartya Majumder
Bhumi N Deshpande
Akash Patel
```
---

